import os
import logging
import requests
import datetime
from functools import lru_cache
import google.auth
import google.auth.transport.requests
import google.oauth2.id_token
from google.cloud import bigquery
from google.cloud import storage
from google.api_core.exceptions import NotFound, Forbidden
import vertexai
from vertexai.generative_models import GenerativeModel
from dotenv import load_dotenv

# --- ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# ==========================================
# ç’°å¢ƒå¤‰æ•°ã®å–å¾—
# ==========================================

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ (ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã®ã¿æœ‰åŠ¹)
load_dotenv()
SAAS_PROJECT_ID = os.getenv("SAAS_PROJECT_ID")
CUSTOMER_PROJECT_ID = os.getenv("CUSTOMER_PROJECT_ID")
BQ_ANTIPATTERN_ANALYZER_URL = os.getenv("BQ_ANTIPATTERN_ANALYZER_URL")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
LOCATION = "us-central1"        # Vertex AIã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³
# èª¿æŸ»æœŸé–“ã®ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—
TIME_RANGE_INTERVAL = os.getenv("TIME_RANGE_INTERVAL")
TIME_RANGE_START = os.getenv("TIME_RANGE_START")
TIME_RANGE_END = os.getenv("TIME_RANGE_END")
# æŠ½å‡ºã™ã‚‹ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®ä»¶æ•°ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ï¼‰
WORST_QUERY_LIMIT = int(os.getenv("WORST_QUERY_LIMIT", "1"))
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
WORST_RANKING_SQL_PATH = os.path.join(BASE_DIR, "sql", "worst_ranking.sql")
STORAGE_ANALYSIS_SQL_PATH = os.path.join(BASE_DIR, "sql", "logical_vs_physical_storage_analysis.sql")
GEMINI_PROMPT_PATH = os.path.join(BASE_DIR, "prompts", "gemini_prompt.txt")

# ==========================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
# ==========================================

def load_external_file(filepath):
    """å¤–éƒ¨SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")
    with open(filepath, "r", encoding="utf-8") as f:
        return f.read()

def get_current_user_email(client):
    """å®Ÿè¡Œè€…ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—ï¼ˆé™¤å¤–ç”¨ï¼‰"""
    try:
        job = client.query("SELECT session_user() as user_email")
        result = list(job.result())
        return result[0].user_email
    except Exception as e:
        logger.warning(f"Could not detect analyzer email: {e}")
        return "unknown"

def get_active_regions(client, target_project):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç‰¹å®š"""
    regions = set()
    logger.info(f"Discovering active regions in {target_project}...")
    try:
        datasets = list(client.list_datasets(project=target_project))
        for dataset_item in datasets:
            dataset = client.get_dataset(dataset_item.reference)
            if dataset.location:
                regions.add(dataset.location.lower())
        return regions
    except Exception as e:
        logger.error(f"Error discovering regions: {e}")
        return set()

def get_time_range_expressions():
    """èª¿æŸ»æœŸé–“ã®æ¡ä»¶å¼ã‚’çµ„ã¿ç«‹ã¦ã‚‹"""
    if TIME_RANGE_INTERVAL:
        start_time_expr = f"TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {TIME_RANGE_INTERVAL})"
        end_time_expr = ""
    elif TIME_RANGE_START:
        start_time_expr = f"TIMESTAMP('{TIME_RANGE_START}')"
        if TIME_RANGE_END:
            end_time_expr = f"AND creation_time <= TIMESTAMP('{TIME_RANGE_END}')"
        else:
            end_time_expr = ""
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š (1æ—¥å‰)
        start_time_expr = "TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)"
        end_time_expr = ""
    return start_time_expr, end_time_expr

# ==========================================
# å¤–éƒ¨API / BigQuery è§£æç³»é–¢æ•°
# ==========================================

@lru_cache(maxsize=1)
def get_oidc_token(audience):
    """OIDCãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ï¼ˆé«˜é€ŸåŒ–ï¼‰"""
    auth_req = google.auth.transport.requests.Request()
    try:
        # æœ¬ç•ªç’°å¢ƒ (Cloud Run) ç”¨
        return google.oauth2.id_token.fetch_id_token(auth_req, audience)
    except Exception:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç’°å¢ƒç”¨
        credentials, _ = google.auth.default()
        credentials.refresh(auth_req)
        return credentials.id_token

def analyze_with_bq_antipattern_analyzer(query_string):
    """æ§‹æ–‡è§£æAPIã‚’å‘¼ã³å‡ºã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã€‚"""
    if not BQ_ANTIPATTERN_ANALYZER_URL:
        logger.warning("BQ_ANTIPATTERN_ANALYZER_URL is not set. Skipping analyzer API call.")
        return "API URLæœªè¨­å®šã®ãŸã‚è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"

    endpoint = f"{BQ_ANTIPATTERN_ANALYZER_URL.rstrip('/')}/analyze"

    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸé–¢æ•°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
        id_token = get_oidc_token(BQ_ANTIPATTERN_ANALYZER_URL)

        headers = {
            "Authorization": f"Bearer {id_token}",
            "Content-Type": "application/json"
        }
        response = requests.post(endpoint, json={"query": query_string}, headers=headers, timeout=60)
        response.raise_for_status()

        return response.json().get("recommendations", "")

    except Exception as e:
        logger.error(f"bq-antipattern-analyzer API call failed: {e}")
        return "ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£æãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def get_query_schema_info(client, referenced_tables):
    """INFORMATION_SCHEMA.JOBSã®å±¥æ­´(referenced_tables)ã‹ã‚‰å…ƒã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å®Œå…¨ãªã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    schema_details = []
    try:
        # å±¥æ­´ã‹ã‚‰å–å¾—ã—ãŸã€Œå‚ç…§ã—ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒªã‚¹ãƒˆã€ã‚’ç¢ºèª
        if not referenced_tables:
            return "å‚ç…§ã—ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        for table_ref in referenced_tables:
            try:
                # table_ref ã¯ dict ã¾ãŸã¯ Row ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æ‰±ã†
                if isinstance(table_ref, dict):
                    project_id = table_ref.get("project_id")
                    dataset_id = table_ref.get("dataset_id")
                    table_id = table_ref.get("table_id")
                else:
                    project_id = getattr(table_ref, "project_id", None)
                    dataset_id = getattr(table_ref, "dataset_id", None)
                    table_id = getattr(table_ref, "table_id", None)

                if not project_id or not dataset_id or not table_id:
                    continue

                table_name = f"{project_id}.{dataset_id}.{table_id}"
                table = client.get_table(table_name)
                info = [f"â–  ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}"]

                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±
                if table.time_partitioning:
                    part_field = table.time_partitioning.field or "_PARTITIONTIME"
                    info.append(f"  - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—: {part_field} (åˆ†å‰²ã‚¿ã‚¤ãƒ—: {table.time_partitioning.type_})")
                else:
                    info.append("  - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³: æœªè¨­å®š (ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®ãƒªã‚¹ã‚¯ã‚ã‚Š)")

                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±
                if table.clustering_fields:
                    info.append(f"  - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ—: {', '.join(table.clustering_fields)}")

                columns = [f"{f.name} ({f.field_type})" for f in table.schema]
                info.append(f"  - ã‚«ãƒ©ãƒ ä¸€è¦§: {', '.join(columns)}")

                schema_details.append("\n".join(info))

            except Exception as e:
                logger.warning(f"Failed to get schema for {table_id}: {e}")
                schema_details.append(f"â–  ãƒ†ãƒ¼ãƒ–ãƒ«: {table_id} (æ¨©é™ä¸è¶³ç­‰ã«ã‚ˆã‚Šã‚¹ã‚­ãƒ¼ãƒå–å¾—å¤±æ•—)")

        return "\n\n".join(schema_details) if schema_details else "å‚ç…§ã—ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    except Exception as e:
        logger.warning(f"Schema extraction failed: {e}")
        return "ã‚¯ã‚¨ãƒªã®è§£æã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

def analyze_storage_pricing(client, target_project, region, sql_template):
    """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ–™é‡‘ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®š"""
    try:
        # formatãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦å¤–éƒ¨SQLã®å¤‰æ•°ã‚’å‹•çš„ã«ç½®æ›
        formatted_sql = sql_template.format(
            target_project=target_project,
            region=region
        )
        query_job = client.query(formatted_sql, location=region)
        results = list(query_job.result())

        if not results:
            return "å¯¾è±¡ã¨ãªã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        # è¡¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ã¯å³å¯„ã› --: ã‚’ä½¿ç”¨ï¼‰
        lines = [
            "| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | è«–ç† (GB) | ç‰©ç† (GB) | åœ§ç¸®ç‡ | æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |",
            "|---|--:|--:|--:|---|"
        ]
        # å–å¾—ã—ãŸçµæœã‚’è¡¨ã®è¡Œã¨ã—ã¦è¿½åŠ 
        for row in results:
            lines.append(
                f"| `{row.dataset_name}` | {row.logical_gb:.2f} | {row.physical_gb:.2f} "
                f"| {row.compression_ratio:.2f} | *{row.recommendation}* |"
            )
        return "\n".join(lines)

    except Exception as e:
        logger.error(f"Storage analysis failed: {e}")
        return "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"


# ==========================================
# ãƒã‚¹ã‚¿ãƒ¼è¾æ›¸ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆãƒ»é€šçŸ¥ç³»é–¢æ•°
# ==========================================

def load_master_dictionary(client, saas_project_id):
    """ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚¹ã‚¿ãƒ¼ã®èª­ã¿è¾¼ã¿"""
    logger.info("Loading anti-pattern master dictionary from BigQuery...")
    master_dict = {}
    query = f"""
        SELECT pattern_name, problem_description, best_practice
        FROM `{saas_project_id}.audit_master.antipattern_master`
    """
    try:
        query_job = client.query(query)
        for row in query_job:
            master_dict[row.pattern_name] = (
                f"â–  {row.pattern_name}\n"
                f"  - å•é¡Œç‚¹: {row.problem_description}\n"
                f"  - ä¿®æ­£ã®å®šçŸ³: {row.best_practice}"
            )
        logger.info(f"Loaded {len(master_dict)} patterns into memory.")
        return master_dict
    except Exception as e:
        logger.error(f"Failed to load master dictionary: {e}")
        return {}

def extract_relevant_dictionary(master_dict, detected_text):
    """æ¤œå‡ºã•ã‚ŒãŸã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿æŠ½å‡º"""
    if not detected_text or not master_dict:
        return "ç‰¹ã«ãªã—"

    relevant_texts = [text for key, text in master_dict.items() if key in detected_text]
    return "\n\n".join(relevant_texts) if relevant_texts else "ç‰¹ã«ãªã—"

def build_gemini_prompt(job, schema_info_text, antipattern_raw_text, master_dict_text):
    """å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿ã€å¤‰æ•°ã‚’æ³¨å…¥ã™ã‚‹"""
    try:
        template = load_external_file(GEMINI_PROMPT_PATH)
        params = {
            "billed_gb": job.billed_gb if job.billed_gb is not None else 0.0,
            "duration_seconds": job.duration_seconds if job.duration_seconds is not None else 0,
            "slot_hours": job.slot_hours if job.slot_hours is not None else 0.0,
            "source_type": job.source_type,
            "difficulty": job.difficulty,
            "query": job.query,
            "schema_info_text": schema_info_text,
            "antipattern_raw_text": antipattern_raw_text,
            "master_dict_text": master_dict_text
        }
        # template.format() ã‚’ä½¿ã„ã€{} ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«è¾æ›¸ã®ä¸­èº«ã‚’æµã—è¾¼ã‚€
        return template.format(**params)

    except Exception as e:
        logger.error(f"Failed to build prompt from external file: {e}")
        return f"Analyze this SQL: {job.query}"

def upload_report_to_gcs(bucket_name, report_content, project_id):
    """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’GCSã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    if not bucket_name:
        logger.warning("GCS_BUCKET_NAME is not set. Skipping GCS upload.")
        return None
    try:
        # GCSãƒã‚±ãƒƒãƒˆã¯CUSTOMER_PROJECT_IDã«å­˜åœ¨ã™ã‚‹å‰æ
        storage_client = storage.Client(project=project_id)
        bucket = storage_client.bucket(bucket_name)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"bq_audit_report_{project_id}_{timestamp}.md"
        blob = bucket.blob(filename)

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob.upload_from_string(report_content, content_type="text/markdown")

        # Cloud Consoleä¸Šã®è©²å½“ãƒ•ã‚¡ã‚¤ãƒ«é–²è¦§URLã‚’è¿”ã™ï¼ˆCUSTOMER_PROJECT_IDã‚’æŒ‡å®šï¼‰
        return f"https://console.cloud.google.com/storage/browser/_details/{bucket_name}/{filename}?project={project_id}"
    except Exception as e:
        logger.error(f"Failed to upload report to GCS: {e}")
        return None

def send_to_slack(text):
    """Slackã¸ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹"""
    if not SLACK_WEBHOOK_URL:
        logger.warning("SLACK_WEBHOOK_URL is not set. Skipping Slack notification.")
        return
    try:
        response = requests.post(SLACK_WEBHOOK_URL, json={"text": text}, timeout=10)
        response.raise_for_status()
    except Exception as e:
        logger.error(f"Failed to send message to Slack: {e}")


# ==========================================
# ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹
# ==========================================

def main():
    if not SAAS_PROJECT_ID or not CUSTOMER_PROJECT_ID:
        logger.error("Environment variables SAAS_PROJECT_ID or CUSTOMER_PROJECT_ID are not set.")
        return

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    bq_client = bigquery.Client(project=SAAS_PROJECT_ID)
    customer_bq_client = bigquery.Client(project=CUSTOMER_PROJECT_ID)
    vertexai.init(project=SAAS_PROJECT_ID, location=LOCATION)
    model = GenerativeModel("gemini-2.5-flash")

    # å¤–éƒ¨SQLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    try:
        worst_ranking_sql_template = load_external_file(WORST_RANKING_SQL_PATH)
        storage_analysis_sql_template = load_external_file(STORAGE_ANALYSIS_SQL_PATH)
    except Exception as e:
        logger.error(f"SQL file loading error: {e}")
        return

    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    analyzer_email = get_current_user_email(bq_client)
    # Cloud Run ã§ã¯ K_SERVICE ç’°å¢ƒå¤‰æ•°ãŒã‚»ãƒƒãƒˆã•ã‚Œã‚‹ãŸã‚ã€ãã‚Œã‚’åˆ©ç”¨ã—ã¦åˆ¤å®š
    if os.getenv("K_SERVICE"):
        exec_env = "Cloud Run"
    else:
        exec_env = "Local"
    logger.info(f"Execution Environment : {exec_env}")
    logger.info(f"Execution Account     : {analyzer_email} (To be excluded)")
    master_dict = load_master_dictionary(bq_client, SAAS_PROJECT_ID)
    target_regions = get_active_regions(customer_bq_client, CUSTOMER_PROJECT_ID)

    if not target_regions:
        logger.info("No active regions found.")
        return

    start_time_expr, end_time_expr = get_time_range_expressions()

    # ãƒ¬ãƒãƒ¼ãƒˆç”¨ãƒªã‚¹ãƒˆï¼ˆæ–‡å­—åˆ—çµåˆã®æœ€é©åŒ–ï¼‰
    report_lines = []
    report_lines.append("# BigQuery ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append(f"**å¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ:** `{CUSTOMER_PROJECT_ID}`")
    report_lines.append(f"**ä½œæˆæ—¥æ™‚:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("\n---")

    all_jobs = []
    storage_proposals = []

    # 1. å„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿åé›†
    for region in target_regions:
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆ†æ
        proposal = analyze_storage_pricing(bq_client, CUSTOMER_PROJECT_ID, region, storage_analysis_sql_template)
        if "å¯¾è±¡ã¨ãªã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" not in proposal and "å¤±æ•—ã—ã¾ã—ãŸ" not in proposal:
            storage_proposals.append(f"### ğŸ“ Region: {region}\n\n{proposal}\n")
        logger.info(f"[{region}] Start extracting the worst queries...")

        # ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒªæŠ½å‡º
        formatted_sql = worst_ranking_sql_template.format(
            target_project=CUSTOMER_PROJECT_ID,
            region=region,
            analyzer_email=analyzer_email,
            start_time_expr=start_time_expr,
            end_time_expr=end_time_expr,
            limit=WORST_QUERY_LIMIT
        )
        try:
            # ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦INFORMATION_SCHEMAã‚’å–å¾—
            query_job = bq_client.query(formatted_sql, location=region)
            all_jobs.extend(list(query_job.result()))
        except Exception as e:
            logger.error(f"Error in {region}: {e}")

    # 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã§ã®ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒªã«çµã‚Šè¾¼ã‚€
    job_ranks = {}
    if all_jobs:
        # --- å…¨ä½“ã®é †ä½ã‚’è¨ˆç®—ã—ã¦ä¿å­˜ ---
        full_sorted_by_billed = sorted(all_jobs, key=lambda x: x.billed_gb or 0.0, reverse=True)
        full_sorted_by_duration = sorted(all_jobs, key=lambda x: x.duration_seconds or 0, reverse=True)
        for rank, j in enumerate(full_sorted_by_billed, 1):
            if j.job_id not in job_ranks:
                job_ranks[j.job_id] = {}
            job_ranks[j.job_id]['cost_rank'] = rank
        for rank, j in enumerate(full_sorted_by_duration, 1):
            job_ranks[j.job_id]['duration_rank'] = rank
        # ------------------------------

        # ã‚¹ã‚­ãƒ£ãƒ³å®¹é‡ã¨å®Ÿè¡Œæ™‚é–“ã®ä¸¡æ–¹ã§ãƒ¯ãƒ¼ã‚¹ãƒˆãªã‚¯ã‚¨ãƒªã‚’ãã‚Œãã‚ŒæŠ½å‡º
        worst_by_billed = sorted(all_jobs, key=lambda x: x.billed_gb or 0.0, reverse=True)[:WORST_QUERY_LIMIT]
        worst_by_duration = sorted(all_jobs, key=lambda x: x.duration_seconds or 0, reverse=True)[:WORST_QUERY_LIMIT]

        # job_idã‚’ã‚­ãƒ¼ã«ã—ã¦é‡è¤‡ã‚’æ’é™¤ã—ã¤ã¤çµåˆ
        final_worst_jobs = {}
        for job in worst_by_billed + worst_by_duration:
            final_worst_jobs[job.job_id] = job

        all_jobs = list(final_worst_jobs.values())
        logger.info(f"Filtered down to project-wide worst queries: {len(all_jobs)} queries.")

    # 3. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆ¤å®šçµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã«è¿½åŠ 
    if storage_proposals:
        report_lines.append("## ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ–™é‡‘ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®šçµæœ\n")
        report_lines.append("\n".join(storage_proposals))
        report_lines.append("---\n")
    else:
        logger.info("No valid storage data to report.")

    # 4. ã‚¸ãƒ§ãƒ–ãŒãªã‘ã‚Œã°çµ‚äº†
    if not all_jobs:
        logger.info("No queries to analyze.")
        report_lines.append("å¯¾è±¡ã®ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")
        final_report = "\n".join(report_lines)
        gcs_url = upload_report_to_gcs(GCS_BUCKET_NAME, final_report, CUSTOMER_PROJECT_ID)
        if gcs_url:
            send_to_slack(f"âœ… *æœ¬æ—¥ã® BigQuery ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚*\nè©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdownï¼‰ã¯ã“ã¡ã‚‰ã®ãƒªãƒ³ã‚¯ã‹ã‚‰ç¢ºèªã§ãã¾ã™:\n{gcs_url}")
        return

    # 5. å„ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®è§£æ
    report_lines.append(f"## ğŸš¨ ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒªè§£æï¼ˆè¨ˆ {len(all_jobs)} ä»¶ï¼‰\n")

    # 6. å„ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦è§£æã¨Geminiç”Ÿæˆã‚’å®Ÿè¡Œ
    for i, job in enumerate(all_jobs, 1):
        logger.info(f"Analyzing Job {i}/{len(all_jobs)}: {job.job_id} ({job.region_name})")
        logger.info("Extracting schema...")

        # ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã®å–å¾— (ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã®ä»£ã‚ã‚Šã«ã‚¸ãƒ§ãƒ–å±¥æ­´ã® referenced_tables ã‚’æ¸¡ã™)
        schema_info_text = get_query_schema_info(bq_client, getattr(job, "referenced_tables", []))
        # æ§‹æ–‡è§£æãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—
        antipattern_raw_text = analyze_with_bq_antipattern_analyzer(job.query)
        # ãƒ¡ãƒ¢ãƒªä¸Šã®è¾æ›¸ã‹ã‚‰å¿…è¦ãªãƒ«ãƒ¼ãƒ«ã ã‘ã‚’å³åº§ã«æŠ½å‡º
        master_dict_text = extract_relevant_dictionary(master_dict, antipattern_raw_text)
        # Geminiã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ(å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å¤‰æ•°æ³¨å…¥)
        prompt = build_gemini_prompt(job, schema_info_text, antipattern_raw_text, master_dict_text)

        try:
            response = model.generate_content(prompt)
            logger.info(f"Gemini Response for Job {job.job_id}:\n{response.text}\n{'-'*50}")
            report_lines.append(f"### ğŸ” ãƒ¯ãƒ¼ã‚¹ãƒˆã‚¯ã‚¨ãƒª {i}/{len(all_jobs)} (Job: `{job.job_id}`)\n")

            # --- ãƒ©ãƒ³ã‚­ãƒ³ã‚°æƒ…å ±ã®è¿½è¨˜ ---
            ranks = job_ranks.get(job.job_id, {})
            cost_rank = ranks.get('cost_rank', '-')
            duration_rank = ranks.get('duration_rank', '-')
            report_lines.append(f"**ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘**\n- ã‚¹ã‚­ãƒ£ãƒ³é‡: ãƒ¯ãƒ¼ã‚¹ãƒˆ **{cost_rank}ä½**\n- å®Ÿè¡Œæ™‚é–“: ãƒ¯ãƒ¼ã‚¹ãƒˆ **{duration_rank}ä½**\n")
            # ---------------------------

            report_lines.append(response.text)
            report_lines.append("\n---")
        except Exception as e:
            logger.error(f"Failed to generate content from Gemini for Job {job.job_id}: {e}")

    # 7. ãƒ¬ãƒãƒ¼ãƒˆã®çµåˆã¨å‡ºåŠ›
    final_report = "\n".join(report_lines)
    gcs_url = upload_report_to_gcs(GCS_BUCKET_NAME, final_report, CUSTOMER_PROJECT_ID)

    if gcs_url:
        send_to_slack(f"âœ… *æœ¬æ—¥ã® BigQuery ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚*\nè©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdownï¼‰ã¯ã“ã¡ã‚‰ã®ãƒªãƒ³ã‚¯ã‹ã‚‰ç¢ºèªã§ãã¾ã™:\n{gcs_url}")
    else:
        send_to_slack("âœ… *æœ¬æ—¥ã® BigQuery ç›£æŸ»ãƒ¬ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚*\n(â€»GCSã¸ã®ä¿å­˜ã«å¤±æ•—ã—ãŸã‹ã€ãƒã‚±ãƒƒãƒˆãŒæœªè¨­å®šã§ã™)")

if __name__ == "__main__":
    main()